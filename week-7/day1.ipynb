{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture Plan\n",
    "\n",
    "1. AutoGen\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain vs AutoGen\n",
    "\n",
    "\n",
    "**LangChain:**\n",
    "\n",
    "1. LLM based applications with Chains, Tools and Agents\n",
    "2. LangChain natievly supports to multiple external data sources\n",
    "3. LangChain uses a Chain Based approach. \n",
    "4. LangChain recently launched LangGraph which supports DAG in Chains and extends LangChain's existing features to Multi Agent workflows.\n",
    "\n",
    "**AutoGen**\n",
    "\n",
    "1. Multi Agent Framework from Microsoft.\n",
    "2. Used to build LLM Applications with workflows involving the Multi Agent Conversations.\n",
    "3. AutoGen uses an Agent Based  Approach\n",
    "4. AutoGen recently launched AutoGen studio. It is a web interface for rapid prototyping.\n",
    "\n",
    "![AutoGen Framework](./images/autogen.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pyautogen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config List\n",
    "\n",
    "- specify the model and other configuration info to agent.\n",
    "\n",
    "**ways to generate config list:**\n",
    "\n",
    "1. `get_config_list`: generates config list for API calls\n",
    "2. `config_list_openai_aoai`:  generates config list for OpenAI and Azure OpenAI endpoints\n",
    "3. `config_list_from_json`: loads configuration from a JSON file. `filter_dict` is used to filter the `config_list_from_json` file.\n",
    "4. `config_list_from_models`: creates configurations based on provided list of models.\n",
    "5. `config_list_from_dotenv`: loads configuration from a .env file. `filter_dict` is used to filter the `config_list_from_json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import config_list_from_dotenv\n",
    "\n",
    "config_list = config_list_from_dotenv(\n",
    "    dotenv_file_path='C:/TrainingMaterial/generative-ai/genai-material/.env',\n",
    "    filter_dict={\n",
    "        \"model\":{\n",
    "            \"gpt-3.5-turbo\"\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agents\n",
    "\n",
    "AutoGen provides an abstraction for creating the agents with following features:\n",
    "\n",
    "- **Conversable:** Any agent can send or recieve messages from the other agent.\n",
    "- **Customizable:** Agents in autogen can be integrated with LLMs, tools and humans.\n",
    "\n",
    "#### Multi Agent Conversational Framework\n",
    "\n",
    "![AutoGen Agents](./images/autogen_agents.png)\n",
    "\n",
    "\n",
    "1. `ConversableAgent`: Genric class for agents that converse with each other by exchnaging messages.\n",
    "2. `AssistantAgent`: designed to act as an AI Assistant using LLMs.\n",
    "3. `UserProxyAgent`: proxy for humans.\n",
    "4. `GroupChatManager` : A chat manager that can manage a group chat of multiple agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code generation agent\n",
    "\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "assistant = AssistantAgent(\n",
    "    name = \"assistant\",\n",
    "    llm_config={\n",
    "        \"config_list\": config_list,\n",
    "        \"temperature\": 0.0,\n",
    "        \"cache_seed\": 42\n",
    "\n",
    "    },\n",
    "    code_execution_config={\n",
    "        \"use_docker\" : False\n",
    "    },\n",
    "    max_consecutive_auto_reply= 10\n",
    ")\n",
    "\n",
    "\n",
    "user_proxy = UserProxyAgent(\n",
    "\n",
    "    name = \"user_proxy\",\n",
    "    system_message=\"reply TERMINATE when you are done with the code\",\n",
    "    human_input_mode=\"TERMINATE\",\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"C:/TrainingMaterial/generative-ai/genai-material/trng-1855/trng-git-repo/trng-1855/week-7/\",\n",
    "        \"use_docker\" : False\n",
    "    },\n",
    "    llm_config={\n",
    "        \"config_list\": config_list,\n",
    "        \"temperature\": 0.0,\n",
    "        \"cache_seed\": 42\n",
    "\n",
    "    },\n",
    "    is_termination_msg = lambda x : x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    max_consecutive_auto_reply= 10\n",
    "    \n",
    ")\n",
    "\n",
    "# response = user_proxy.initiate_chat(\n",
    "#     assistant,\n",
    "#     message = \"\"\"Create a Langchain agent with RAG\"\"\",\n",
    "#     summry_method=\"reflection_from_llms\"\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_manager = AssistantAgent(\n",
    "    name = \"product_manager\",\n",
    "    llm_config={\n",
    "        \"config_list\": config_list,\n",
    "        \"temperature\": 0.0,\n",
    "        \"cache_seed\": 42\n",
    "\n",
    "    },\n",
    "    code_execution_config={\n",
    "        \"use_docker\" : False\n",
    "    },\n",
    "    max_consecutive_auto_reply= 10,\n",
    "    system_message= \"Agent to ask unique coding questions as a continuation for user questions\"    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Code to search a number in a list?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Sure! Here's an example code to search for a number in a list:\n",
      "\n",
      "```python\n",
      "def search_number(numbers, target):\n",
      "    for number in numbers:\n",
      "        if number == target:\n",
      "            return True\n",
      "    return False\n",
      "\n",
      "# Example usage\n",
      "numbers = [1, 2, 3, 4, 5]\n",
      "target = 3\n",
      "\n",
      "if search_number(numbers, target):\n",
      "    print(f\"The number {target} is found in the list.\")\n",
      "else:\n",
      "    print(f\"The number {target} is not found in the list.\")\n",
      "```\n",
      "\n",
      "In this code, the `search_number` function takes in a list of numbers and a target number to search for. It iterates through each number in the list and checks if it matches the target number. If a match is found, it returns `True`. If no match is found after iterating through the entire list, it returns `False`.\n",
      "\n",
      "In the example usage, we define a list of numbers `[1, 2, 3, 4, 5]` and a target number `3`. We call the `search_number` function with these arguments and check the returned value. If the number is found, it prints a message indicating that the number is found in the list. Otherwise, it prints a message indicating that the number is not found in the list.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autogen.code_utils:SIGALRM is not supported on Windows. No timeout will be enforced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mproduct_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "The number 3 is found in the list.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Great! I'm glad the code worked for you. If you have any more questions or need further assistance, feel free to ask.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Agent Group Chat\n",
    "\n",
    "from autogen import GroupChat, GroupChatManager\n",
    "\n",
    "\n",
    "agent_group = GroupChat(agents=[assistant, user_proxy, product_manager], messages = [], max_round = 10)\n",
    "\n",
    "manager = GroupChatManager(\n",
    "    groupchat= agent_group,\n",
    "    max_consecutive_auto_reply= 10,\n",
    "    llm_config={\n",
    "        \"config_list\": config_list,\n",
    "        \"temperature\": 0.0,\n",
    "        \"cache_seed\": 42\n",
    "\n",
    "    },\n",
    "    code_execution_config={\n",
    "        \"use_docker\":False\n",
    "        \n",
    "    }\n",
    ")\n",
    "\n",
    "result = user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    message = \"Code to search a number in a list?\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "\n",
    "- [AutoGen Docs](https://microsoft.github.io/autogen/docs/Getting-Started)\n",
    "- [AutoGen SDK](https://microsoft.github.io/autogen/docs/reference/agentchat/conversable_agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
