{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture Plan\n",
    "\n",
    "QC Audit & Review\n",
    "\n",
    "Project 1 Presentations\n",
    "\n",
    "1. Langchain Introduction\n",
    "2. LLM and Chat Models\n",
    "3. Prompt Template\n",
    "4. Prompt Serialization\n",
    "5. Model\n",
    "6. Output Parser\n",
    "7. Chains\n",
    "   - LLM Chain\n",
    "   - Simple Sequenctial Chain\n",
    "   - Sequential Chain\n",
    "   - Router Chain\n",
    "   - Math Chain\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain\n",
    "\n",
    "- Langchain is an open source library for building llm applications.\n",
    "- supports python and JavaScript\n",
    "\n",
    "![LangChain Stack](./images/langchain-stack.svg)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM and Chat Model\n",
    "\n",
    "In Langchain there is small but significant difference between llms and chat models. \n",
    "\n",
    "**LLM:** Large language model. text in text out API.\n",
    "\n",
    "**Chat Model:** It  uses LLM for a more conversational approach. It is an interface around messages. The current supported messages in Langchain are: SystemMessage, HumanMessage, AIMessage, FunctionMessage and ChatMessage.\n",
    "\n",
    "**Chain:**  Chain is composition of langchain elements. \n",
    "\n",
    "Ex: A simple LLM Chain consists of Prompt | Model | Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain langchain_experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template\n",
    "\n",
    "an abstraction for prompt template\n",
    "\n",
    "[Prompt Templates](https://python.langchain.com/docs/modules/model_io/prompts/quick_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'PromptTemplate',\n",
       " 'description': 'A prompt template for a language model.\\n\\nA prompt template consists of a string template. It accepts a set of parameters\\nfrom the user that can be used to generate a prompt for a language model.\\n\\nThe template can be formatted using either f-strings (default) or jinja2 syntax.\\n\\n*Security warning*: Prefer using `template_format=\"f-string\"` instead of\\n    `template_format=\"jinja2\"`, or make sure to NEVER accept jinja2 templates\\n    from untrusted sources as they may lead to arbitrary Python code execution.\\n\\n    As of LangChain 0.0.329, Jinja2 templates will be rendered using\\n    Jinja2\\'s SandboxedEnvironment by default. This sand-boxing should\\n    be treated as a best-effort approach rather than a guarantee of security,\\n    as it is an opt-out rather than opt-in approach.\\n\\n    Despite the sand-boxing, we recommend to never use jinja2 templates\\n    from untrusted sources.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.prompts import PromptTemplate\\n\\n        # Instantiation using from_template (recommended)\\n        prompt = PromptTemplate.from_template(\"Say {foo}\")\\n        prompt.format(foo=\"bar\")\\n\\n        # Instantiation using initializer\\n        prompt = PromptTemplate(input_variables=[\"foo\"], template=\"Say {foo}\")',\n",
       " 'type': 'object',\n",
       " 'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
       "  'input_variables': {'title': 'Input Variables',\n",
       "   'type': 'array',\n",
       "   'items': {'type': 'string'}},\n",
       "  'input_types': {'title': 'Input Types', 'type': 'object'},\n",
       "  'output_parser': {'$ref': '#/definitions/BaseOutputParser'},\n",
       "  'template': {'title': 'Template', 'type': 'string'},\n",
       "  'template_format': {'title': 'Template Format',\n",
       "   'default': 'f-string',\n",
       "   'enum': ['f-string', 'jinja2'],\n",
       "   'type': 'string'},\n",
       "  'validate_template': {'title': 'Validate Template',\n",
       "   'default': False,\n",
       "   'type': 'boolean'}},\n",
       " 'required': ['input_variables', 'template'],\n",
       " 'definitions': {'BaseOutputParser': {'title': 'BaseOutputParser',\n",
       "   'description': 'Base class to parse the output of an LLM call.\\n\\nOutput parsers help structure language model responses.\\n\\nExample:\\n    .. code-block:: python\\n\\n        class BooleanOutputParser(BaseOutputParser[bool]):\\n            true_val: str = \"YES\"\\n            false_val: str = \"NO\"\\n\\n            def parse(self, text: str) -> bool:\\n                cleaned_text = text.strip().upper()\\n                if cleaned_text not in (self.true_val.upper(), self.false_val.upper()):\\n                    raise OutputParserException(\\n                        f\"BooleanOutputParser expected output value to either be \"\\n                        f\"{self.true_val} or {self.false_val} (case-insensitive). \"\\n                        f\"Received {cleaned_text}.\"\\n                    )\\n                return cleaned_text == self.true_val.upper()\\n\\n                @property\\n                def _type(self) -> str:\\n                        return \"boolean_output_parser\"',\n",
       "   'type': 'object',\n",
       "   'properties': {'name': {'title': 'Name', 'type': 'string'}}}}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PromptTemplate\n",
    "\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "\n",
    "# template_string=\"\"\"\n",
    "# <system>\\\n",
    "\n",
    "# </system>\n",
    "\n",
    "# content: {context}\n",
    "\n",
    "# <user> {user}</user>\n",
    "\n",
    "# answer:\n",
    "# \"\"\"\n",
    "\n",
    "template_string= \"generate code for {text} in {language}\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template=template_string)\n",
    "\n",
    "prompt.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "\n",
    "from langchain.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo HUGGINGFACEHUB_API_TOKEN=token >> .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\TrainingMaterial\\generative-ai\\genai-material\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace API\n",
    "from langchain.llms.huggingface_hub import HuggingFaceHub\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-xxl\",\n",
    "    model_kwargs={\"temperature\":0.0, \"max_length\":200}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat Prompt Template\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant.\"),\n",
    "        (\"human\",\"Hi!\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "template.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\TrainingMaterial\\generative-ai\\genai-material\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Chat Model\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.deepinfra import DeepInfra\n",
    "from langchain_experimental.chat_models import Llama2Chat \n",
    "\n",
    "llm = DeepInfra(\n",
    "    model_id=\"meta-llama/Llama-2-70b-chat-hf\"\n",
    ")\n",
    "\n",
    "llm.model_kwargs = {\n",
    "    \"temperature\":0.0\n",
    "}\n",
    "\n",
    "model = Llama2Chat(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String Output Parser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '\\n\\nimport java.util.Scanner;\\n\\npublic class Addition {\\n    public static void main(String[] args) {\\n        Scanner scanner = new Scanner(System.in);\\n        System.out.print(\"Enter the first number: \");\\n        int num1 = scanner.nextInt();\\n        System.out.print(\"Enter the second number: \");\\n        int num2 = scanner.nextInt();\\n        int sum = add_numbers(num1, num2);\\n        System.out.println(\"The sum is: \" + sum);\\n    }\\n\\n    public static int add_numbers(int num1, int num2) {\\n        // TO DO: implement the logic to add the two numbers\\n        // HINT: you can use the + operator\\n        int sum = num1 + num2;\\n        return sum;\\n    }\\n}\\n```\\nThis code prompts the user to enter two numbers, and then it calls the `add_numbers` method to add those numbers and return the result. Finally, it prints the result to the console.\\n\\nYou need to implement the `add_numbers` method by adding the logic to add the two numbers. In this case, you can simply use the `+` operator to add the two numbers and return the result.\\n\\nHere\\'s an example of how the completed code would look like:\\n```\\nimport java.util.Scanner;\\n\\npublic class Addition {\\n    public static void main(String[] args) {\\n        Scanner scanner = new Scanner(System.in);\\n        System.out.print(\"Enter the first number: \");\\n        int num1 = scanner.nextInt();\\n        System.out.print(\"Enter the second number: \");\\n        int num2 = scanner.nextInt();\\n        int sum = add_numbers(num1, num2);\\n        System.out.println(\"The sum is: \" + sum);\\n    }\\n\\n    public static int add_numbers(int num1, int num2) {\\n        return num1 + num2;\\n    }\\n}\\n```\\nThis code will work as expected and print the sum of the two numbers that the user enters.',\n",
       " 'language': 'java'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    output_parser=output_parser\n",
    ")\n",
    "\n",
    "chain.invoke({\"text\":\"add_numbers\", \"language\":\"java\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON Output Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Expression Language (LCEL)\n",
    "\n",
    "\n",
    "- Easy way to compose chains together using Runnable Interface\n",
    "  \n",
    "Provides:\n",
    "\n",
    "- Streaming\n",
    "- Async\n",
    "- Parallel Execution\n",
    "- Retires and Fallbacks\n",
    "- Input and Output Schemas\n",
    "- Integration with langsmith\n",
    "\n",
    "#### Runnable Interface\n",
    "\n",
    "- strem\n",
    "- invoke\n",
    "- batch\n",
    "\n",
    "Asyn:\n",
    "\n",
    "- astrem\n",
    "- ainvoke\n",
    "- abatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Chains\n",
    "\n",
    "- Chains are composition of langchain elements. \n",
    "\n",
    "Types of Chain\n",
    "\n",
    "1. `LLMChain`: An LLMChain is a simple chain that adds some functionality around language models.\n",
    "\n",
    "An LLMChain consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.\n",
    "\n",
    "\n",
    "from the `Chain` object\n",
    "\n",
    "- __call__ -> dict to dict\n",
    "- run -> keyword arguments/variable argumants to string or llm output\n",
    "\n",
    "other methods include\n",
    "\n",
    "- apply-> list of inputs\n",
    "- generate -> list of inputs returns LLMResult (aditional info like token usage)\n",
    "- predict -> input as keyword args returns string or llm output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Sequential Chain\n",
    "\n",
    "![Simple Sequential Chain](./images/SimpleSequentialChain.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential Chain\n",
    "\n",
    "![Sequential Chain](./images/SequentialChain.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Router Chain\n",
    "\n",
    "![Router Chain](./images/RouterChain.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Math Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
