{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture Plan\n",
    "\n",
    "QC Audit & Review\n",
    "\n",
    "Project 1 Presentations\n",
    "\n",
    "1. Langchain Introduction\n",
    "2. LLM and Chat Models\n",
    "3. Prompt Template\n",
    "4. Prompt Serialization\n",
    "5. Model\n",
    "6. Output Parser\n",
    "7. Chains\n",
    "   - LLM Chain\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain\n",
    "\n",
    "- Langchain is an open source library for building llm applications.\n",
    "- supports python and JavaScript\n",
    "\n",
    "![LangChain Stack](./images/langchain-stack.svg)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM and Chat Model\n",
    "\n",
    "In Langchain there is small but significant difference between llms and chat models. \n",
    "\n",
    "**LLM:** Large language model. text in text out API.\n",
    "\n",
    "**Chat Model:** It  uses LLM for a more conversational approach. It is an interface around messages. The current supported messages in Langchain are: SystemMessage, HumanMessage, AIMessage, FunctionMessage and ChatMessage.\n",
    "\n",
    "**Chain:**  Chain is composition of langchain elements. \n",
    "\n",
    "Ex: A simple LLM Chain consists of Prompt | Model | Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain langchain_experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template\n",
    "\n",
    "an abstraction for prompt template\n",
    "\n",
    "[Prompt Templates](https://python.langchain.com/docs/modules/model_io/prompts/quick_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PromptTemplate\n",
    "\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "\n",
    "# template_string=\"\"\"\n",
    "# <system>\\\n",
    "\n",
    "# </system>\n",
    "\n",
    "# content: {context}\n",
    "\n",
    "# <user> {user}</user>\n",
    "\n",
    "# answer:\n",
    "# \"\"\"\n",
    "\n",
    "template_string= \"generate code for {text} in {language}\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template=template_string)\n",
    "\n",
    "prompt.save('sample_template.json')\n",
    "prompt.save('sample_template.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_prompt = PromptTemplate.from_file('sample_template.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open AI LLM\n",
    "\n",
    "from langchain.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo HUGGINGFACEHUB_API_TOKEN=token >> .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\TrainingMaterial\\generative-ai\\genai-material\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace API\n",
    "from langchain.llms.huggingface_hub import HuggingFaceHub\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-xxl\",\n",
    "    model_kwargs={\"temperature\":0.0, \"max_length\":200}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat Prompt Template\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant.\"),\n",
    "        (\"human\",\"Hi!\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "template.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\TrainingMaterial\\generative-ai\\genai-material\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Chat Model\n",
    "# Open AI\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat Model\n",
    "# Deep Infra\n",
    "\n",
    "from langchain.llms.deepinfra import DeepInfra\n",
    "from langchain_experimental.chat_models import Llama2Chat \n",
    "\n",
    "llm = DeepInfra(\n",
    "    model_id=\"meta-llama/Llama-2-70b-chat-hf\"\n",
    ")\n",
    "\n",
    "llm.model_kwargs = {\n",
    "    \"temperature\":0.0\n",
    "}\n",
    "\n",
    "model = Llama2Chat(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String Output Parser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '\\n\\nimport java.util.Scanner;\\n\\npublic class Addition {\\n    public static void main(String[] args) {\\n        Scanner scanner = new Scanner(System.in);\\n        System.out.print(\"Enter the first number: \");\\n        int num1 = scanner.nextInt();\\n        System.out.print(\"Enter the second number: \");\\n        int num2 = scanner.nextInt();\\n        int sum = add_numbers(num1, num2);\\n        System.out.println(\"The sum is: \" + sum);\\n    }\\n\\n    public static int add_numbers(int num1, int num2) {\\n        // TO DO: implement the logic to add the two numbers\\n        // HINT: you can use the + operator\\n        int sum = num1 + num2;\\n        return sum;\\n    }\\n}\\n```\\nThis code prompts the user to enter two numbers, and then it calls the `add_numbers` method to add those numbers and return the result. Finally, it prints the result to the console.\\n\\nYou need to implement the `add_numbers` method by adding the logic to add the two numbers. In this case, you can simply use the `+` operator to add the two numbers and return the result.\\n\\nHere\\'s an example of how the completed code would look like:\\n```\\nimport java.util.Scanner;\\n\\npublic class Addition {\\n    public static void main(String[] args) {\\n        Scanner scanner = new Scanner(System.in);\\n        System.out.print(\"Enter the first number: \");\\n        int num1 = scanner.nextInt();\\n        System.out.print(\"Enter the second number: \");\\n        int num2 = scanner.nextInt();\\n        int sum = add_numbers(num1, num2);\\n        System.out.println(\"The sum is: \" + sum);\\n    }\\n\\n    public static int add_numbers(int num1, int num2) {\\n        return num1 + num2;\\n    }\\n}\\n```\\nThis code will work as expected and print the sum of the two numbers that the user enters.',\n",
       " 'language': 'java'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM Chain\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    output_parser=output_parser\n",
    ")\n",
    "\n",
    "chain.invoke({\"text\":\"add_numbers\", \"language\":\"java\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
