{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture Plan\n",
    "\n",
    "1. Chains\n",
    "   - Simple Sequenctial Chain\n",
    "   - Sequential Chain\n",
    "   - Router Chain\n",
    "   - Math Chain\n",
    "   - Tranform Chain\n",
    "2. Document Loaders\n",
    "3. Transformers\n",
    "4. Text Embeddings\n",
    "5. Vector Store\n",
    "6. Retreivers\n",
    "7. Multi Query Retreival\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Expression Language (LCEL)\n",
    "\n",
    "\n",
    "- Easy way to compose chains together using Runnable Interface\n",
    "  \n",
    "Provides:\n",
    "\n",
    "- Streaming\n",
    "- Async\n",
    "- Parallel Execution\n",
    "- Retires and Fallbacks\n",
    "- Input and Output Schemas\n",
    "- Integration with langsmith\n",
    "\n",
    "#### Runnable Interface\n",
    "\n",
    "- strem\n",
    "- invoke\n",
    "- batch\n",
    "\n",
    "Async:\n",
    "\n",
    "- astrem\n",
    "- ainvoke\n",
    "- abatch\n",
    "\n",
    "\n",
    "[LCEL Reference](https://python.langchain.com/docs/expression_language/interface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runnable Interface\n",
    "\n",
    "\n",
    "1. Runnable Parallel: used to define  and run multiple values and operations in parallel.\n",
    "\n",
    "2. Runnble Passthrough: used to take the input and pass it through \n",
    "\n",
    "3. Runnable Lambda: used to turn the python functions to pipe compatable functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\TrainingMaterial\\generative-ai\\genai-material\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.deepinfra import DeepInfra\n",
    "from langchain_experimental.chat_models import Llama2Chat\n",
    "llm = DeepInfra(\n",
    "    model_id=\"meta-llama/Llama-2-70b-chat-hf\"\n",
    ")\n",
    "\n",
    "model = Llama2Chat(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a code assistant that generates code for users input\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"provide code for {text} in {language}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure! Here\\'s a code snippet that prints all prime numbers within a given range in Python:\\n\\n```python\\ndef is_prime(num):\\n    if num < 2:\\n        return False\\n    for i in range(2, int(num ** 0.5) + 1):\\n        if num % i == 0:\\n            return False\\n    return True\\n\\nstart = int(input(\"Enter the starting number: \"))\\nend = int(input(\"Enter the ending number: \"))\\n\\nprint(\"Prime numbers between\", start, \"and\", end, \"are:\")\\nfor num in range(start, end + 1):\\n    if is_prime(num):\\n        print(num)\\n```\\n\\nIn this code, we define a helper function `is_prime()` that checks whether a number is prime or not. Then, we take user input for the starting and ending numbers of the range. Finally, we iterate through the range and print all the prime numbers within that range.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM Chain\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({'text':'print prime numbers', 'language':'python'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Sequential Chain\n",
    "\n",
    "![Simple Sequential Chain](./images/SimpleSequentialChain.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_name_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are an assistant that provides a single company name based on the description provided by the user\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\")\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "slogan_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"genrate a slogan from company name provided by the user and print it along with the name of the company\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{company_name}\")\n",
    "\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "chain = RunnableParallel(\n",
    "    company_name = company_name_prompt | model,\n",
    "    company_slogan = {'company_name' : company_name_prompt | model} | slogan_prompt_template | model | parser \n",
    ")\n",
    "# chain = {'company_name' : company_name_prompt | model} | slogan_prompt_template | model | parser \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'company_name': AIMessage(content='  Sure! Based on your description, the company name that comes to mind is \"Terrafugia\". Terrafugia is a company that designs and manufactures flying cars, also known as personal aerial vehicles or PAVs. The company was founded in 2006 and is based in Woburn, Massachusetts, USA. Terrafugia\\'s mission is to create practical and safe flying cars that can be used for personal transportation and recreation. Their first product, the Transition, is a light aircraft that can convert from a car to a plane in just a few minutes.'),\n",
       " 'company_slogan': '  Sure, here\\'s a slogan for Terrafugia:\\n\\n\"Terrafugia: Take to the skies, and hit the road\"\\n\\nOr, if you prefer, here\\'s another option:\\n\\n\"Terrafugia: The sky\\'s the limit, and so is the highway\"\\n\\nI hope these options capture the essence of Terrafugia\\'s innovative technology and the freedom it offers to its customers!'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'text':'a company that sells flying cars'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential Chain\n",
    "\n",
    "![Sequential Chain](./images/SequentialChain.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_generation_prompt = ChatPromptTemplate.from_messages(\n",
    "   [\n",
    "       SystemMessage(content=\"You are a code assistant that generates code for users input\"),\n",
    "       HumanMessagePromptTemplate.from_template(\"generate code for {text} in {language}\")\n",
    "\n",
    "   ] \n",
    ")\n",
    "\n",
    "time_complexity_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a code assistant that evaluates the time and space complexity of code for users input\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{code}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "code_summary_chain = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"you are a code assitant that prints the time complexity of the code and code provided by the user and an explanation for the code\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{code} and {time_complexity} \")\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "chain = RunnableParallel(\n",
    "    code_generation = code_generation_prompt | model,\n",
    "    code_complexity = RunnablePassthrough.assign(code = (code_generation_prompt | model))| time_complexity_prompt | model,\n",
    "    code_comparision = {'code' : (code_generation_prompt | model), 'time_complexity': ({'code' : code_generation_prompt | model} | time_complexity_prompt | model)} | code_summary_chain | model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code_generation': AIMessage(content=\"  Sure! Here's an example code that prints all prime numbers up to a given number `n`:\\n```\\ndef print_primes(n):\\n    for i in range(2, n+1):\\n        is_prime = True\\n        for j in range(2, int(i**0.5) + 1):\\n            if i % j == 0:\\n                is_prime = False\\n                break\\n        if is_prime:\\n            print(i)\\n```\\nHere's an explanation of the code:\\n\\n1. The function `print_primes` takes an integer `n` as input.\\n2. The loop `for i in range(2, n+1)` iterates over the numbers from 2 to `n`.\\n3. The variable `is_prime` is initialized to `True` for each iteration.\\n4. The loop `for j in range(2, int(i**0.5) + 1)` checks whether `i` is divisible by any number between 2 and the square root of `i`. If `i` is divisible by `j`, then `is_prime` is set to `False` and the loop breaks.\\n5. If `is_prime` is still `True` after the inner loop, then `i` is a prime number and it is printed.\\n\\nHere's an example usage of the function:\\n```\\nprint_primes(30)\\n```\\nThis will print all prime numbers up to 30, which are:\\n\\n2, 3, 5, 7, 11, 13, 17, 19, 23, 29\\n\\nI hope this helps! Let me know if you have any questions or need further assistance.\"),\n",
       " 'code_complexity': AIMessage(content='  Sure, I can help you with that!\\n\\nThe first code snippet you provided has a time complexity of O(n^2), where n is the number entered by the user. This is because the inner for loop iterates from 2 to the square root of n, and the outer for loop iterates from 2 to n. The number of iterations in the inner loop increases quadratically with n, so the overall time complexity is O(n^2).\\n\\nThe second code snippet you provided has a time complexity of O(n), where n is the number entered by the user. This is because the for loop iterates from 2 to n, and the `gcd` function has a time complexity of O(1) for each iteration. Therefore, the overall time complexity is O(n).\\n\\nIn terms of space complexity, both code snippets have a space complexity of O(1) because they only use a constant amount of memory, regardless of the size of the input.\\n\\nSo, the second code snippet is more efficient in terms of time complexity, but both code snippets have the same space complexity.'),\n",
       " 'code_comparision': AIMessage(content=\"  Sure, I'd be happy to help! The three codes you provided are all valid ways to find prime numbers up to a certain number, and they each have different time and space complexities.\\n\\n1. The first code uses a for loop to iterate from 2 to n, and then uses another for loop to check if the current number is prime. The time complexity of this code is O(n^2), because the inner loop runs up to sqrt(n) times, and the outer loop runs n times. The space complexity is O(1), because only a small amount of extra memory is needed to store the variables.\\n\\n2. The second code uses the `isprime` function from the `math.gcd` module to check if a number is prime. The time complexity of this code is O(n), because the `isprime` function takes constant time to execute, and the for loop runs n times. The space complexity is also O(1), because only a small amount of extra memory is needed to store the variables.\\n\\n3. The third code uses the Sieve of Eratosthenes algorithm to find all prime numbers up to a certain number. The time complexity of this code is O(n log log n), because the sieve algorithm takes advantage of the fact that the prime numbers are distributed unevenly, and only needs to check a subset of the numbers. The space complexity is O(n), because the sieve array needs to be able to store all the numbers up to n.\\n\\nIn general, the Sieve of Eratosthenes algorithm is the most efficient way to find prime numbers up to a certain number, followed by the `isprime` function from the `math.gcd` module. The first code is the least efficient of the three, because it uses a nested loop that takes quadratic time.\")}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'text':'print prime numbers', 'language':'python'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Router Chain\n",
    "\n",
    "![Router Chain](./images/RouterChain.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_prompt = ChatPromptTemplate.from_messages(\n",
    "\n",
    "    [\n",
    "\n",
    "    SystemMessage(content=\"You are a helpfull, honest sales assisnt for a telecom company called brightspeed\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{text}\")\n",
    "\n",
    "    ]\n",
    "    \n",
    ")\n",
    "\n",
    "service_prompt = ChatPromptTemplate.from_messages(\n",
    "\n",
    "    [\n",
    "    SystemMessage(content=\"You are a helpfull, honest service assisnt for a telecom company called brightspeed guide the user to resolve the issues.\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{text}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "router_prompt = ChatPromptTemplate.from_messages(\n",
    "\n",
    "    [\n",
    "    SystemMessage(content=\"Categorize the user input to sales or service along with actual text\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{text}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "# classifier = pipeline('zero-shot-classification', candidate_labels = ['sales', 'service'])\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = router_prompt | model | parser\n",
    "\n",
    "def categorize_text(text):\n",
    "    if(\"sales\" in text):\n",
    "        return sales_prompt\n",
    "    else:\n",
    "        return service_prompt\n",
    "    \n",
    "\n",
    "router_chain = RunnablePassthrough.assign(text = (router_prompt | model | parser)) | RunnableLambda(lambda text: categorize_text(text)) | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"  Hello! Welcome to Brightspeed! We're happy to help you with your inquiry.\\n\\nWe offer a variety of broadband plans that cater to different needs and budgets. Our plans come with different speeds, data caps, and features to ensure you get the best experience for your online activities.\\n\\nHere are some of our most popular broadband plans:\\n\\n1. Basic: This plan offers speeds of up to 50 Mbps and a data cap of 500 GB. It's perfect for light internet users who primarily use the internet for browsing, emailing, and social media.\\n2. Standard: With speeds of up to 100 Mbps and a data cap of 1 TB, this plan is ideal for households with multiple devices. It's great for streaming, online gaming, and heavy internet usage.\\n3. Premium: Our top-tier plan offers speeds of up to 500 Mbps and a data cap of 2 TB. It's designed for heavy internet users who need fast speeds for streaming, online gaming, and large file transfers.\\n\\nWe also offer customized plans for businesses, so please let us know if you have any specific requirements.\\n\\nBefore choosing a plan, you may want to consider factors such as the number of devices you'll be connecting, the type of online activities you'll be doing, and your budget. Our team can help you determine the best plan for your needs.\\n\\nWould you like me to recommend a plan based on your specific requirements? Or would you like to know more about any of our plans?\")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "router_chain.invoke({'text':\"What are the broadband plans available?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform Chain\n",
    "\n",
    "- used to transform the input before it is passed to the prompt or model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='  Sure, I\\'d be happy to help! The actual text you provided is:\\n\\n\"ITS ME HI!\"\\n\\nIs there anything else I can assist you with?')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_upper(input):\n",
    "    return {'text':str(input).upper()}\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"you are a helpfull assistant\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}, print the actual text\")\n",
    "    ]\n",
    "    \n",
    ") \n",
    "\n",
    "chain = RunnableLambda(lambda text: to_upper(text)) | prompt | model\n",
    "\n",
    "chain.invoke({\"text\":\"Its me Hi!\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Math Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': {'What is 2+2?'}, 'answer': 'Answer: 4'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "chain = LLMMathChain.from_llm(llm)\n",
    "\n",
    "chain.invoke({\"What is 2+2?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Legacy Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Chains\n",
    "\n",
    "from langchain.chains import SimpleSequentialChain, LLMChain\n",
    "\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "chain_one = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=first_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Provide a slogan for the company {company_name}\"\n",
    ")\n",
    "\n",
    "chain_two = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=second_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_seq_chain = SimpleSequentialChain(chains=[chain_one,chain_two], verbose=True)\n",
    "\n",
    "simple_seq_chain.run(\"invisible car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "language_translation_prompt = ChatPromptTemplate.from_template(\n",
    "    \"translate the {review} to english\"\n",
    ")\n",
    "\n",
    "review_prompt=ChatPromptTemplate.from_template(\n",
    "    \"provide a summary for the {english_review} in 20 words\"\n",
    ")\n",
    "\n",
    "language_recognition_prompt = ChatPromptTemplate.from_template(\n",
    "    \"what language is the review {review} in?\"\n",
    ")\n",
    "\n",
    "final_response_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"write a follow up response to the following\n",
    "    summary in the specific langauge: {summary} \\n langauge : {language}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "translation_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=language_translation_prompt,\n",
    "    output_key=\"english_review\",\n",
    ")\n",
    "review_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=review_prompt,\n",
    "    output_key=\"summary\",\n",
    "\n",
    ")\n",
    "language_recognition_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=language_recognition_prompt,\n",
    "    output_key=\"language\",\n",
    ")\n",
    "final_response_chain= LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=final_response_prompt,\n",
    "    output_key=\"followup_message\",\n",
    ")\n",
    "\n",
    "sequential_chain= SequentialChain(\n",
    "    chains=[translation_chain,review_chain,language_recognition_chain,final_response_chain],\n",
    "    input_variables=[\"review\"],\n",
    "    output_variables=[\"english_review\",\"summary\",\"followup_message\",\"language\"],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_chain(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Router chain\n",
    "\n",
    "positive_review_prompt = \"provide a response thanking the user for the {input}\"\n",
    "\n",
    "negative_review_prompt = \"Provide a solution for the {input} and apologise to the customer\"\n",
    "\n",
    "prompt_info = [\n",
    "    {\n",
    "        \"name\":\"thank you response\",\n",
    "        \"description\" :\"A thank you note for positive review from the customer\",\n",
    "        \"prompt_template\":positive_review_prompt\n",
    "    },\n",
    "    {\n",
    "        \"name\":\"apology\",\n",
    "        \"description\" :\"An apology message to the customer for a negative review or statemnt from the customer\",\n",
    "        \"prompt_template\":negative_review_prompt\n",
    "      \n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "destination_chains = {}\n",
    "\n",
    "for p in prompt_info:\n",
    "    name = p['name']\n",
    "    prompt = ChatPromptTemplate.from_template(template=p['prompt_template'])\n",
    "    chain = LLMChain(prompt=prompt, llm=llm)\n",
    "    destination_chains[name]=chain\n",
    "\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_info]\n",
    "destination_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedault_prompt = PromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=dedault_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destination_str)\n",
    "router_prompt = PromptTemplate(template=router_template,input_variables=[\"input\"], output_parser=RouterOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_router_chain = LLMRouterChain.from_llm(llm=llm, prompt=router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_router_chain = MultiPromptChain(router_chain=llm_router_chain,\n",
    "                                      default_chain=default_chain,\n",
    "                                      verbose=True,\n",
    "                                      destination_chains=destination_chains)\n",
    "\n",
    "final_router_chain.run(\"Do you sell potatos?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
