{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture Plan\n",
    "\n",
    "1. AI, ML and Gen AI Introduction\n",
    "2. LLMs\n",
    "3. Hugging face\n",
    "   - Huggingface models\n",
    "   - pipelines and finetuning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Intelligence\n",
    "\n",
    "Development of computer systems to perform tasks that require human Intelligence. Logical resoning, NLP, Problem solving. \n",
    "\n",
    "### Machine Learning\n",
    "\n",
    "- Subset of AI focusing on developing models that enable computers to learn from data without explicit programming.\n",
    "- ML Model:\n",
    "  - data prep\n",
    "  - train\n",
    "  - evaluate\n",
    "- The models performce is directly proportinal to the quality of training data and duration of training.\n",
    "\n",
    "f(x) = wx+ b\n",
    "\n",
    "Types of learning:\n",
    "\n",
    "- Supervised Learning\n",
    "- Unsupervised Learning\n",
    "  - Self Supervised Learning \n",
    "- Semi Supervised Learning\n",
    "\n",
    "### Generative AI\n",
    "\n",
    "Gen AI, generates text, images, videos, code, speech and other media.\n",
    "\n",
    "### LLM\n",
    "\n",
    "- LLMs are Langauge models that are trained to generate the next words or tokens, given some input text.\n",
    "- Natural Language Processing (NLP): Natural Language Underatanding (NLU) + Natural Lnaguage Generation (NLG) \n",
    "\n",
    "#### Examples\n",
    "\n",
    "- Open AI: GPT 2, 3, 3.5 turbo and 4.\n",
    "- Antropic: Claude 1, 2\n",
    "- Google: PaLM text-bision, chat-bision\n",
    "- Meta: LLama, LLama 2\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face\n",
    "\n",
    "- Open source data science and Machine learning platform. It started as a chatbot for teenagers in 2017.\n",
    "- It is used to build, train and deploy the AI models.\n",
    "\n",
    "### transformers library\n",
    "\n",
    "It is a python library maintained by hugging face for Machine Learning for Pytorch, Tensorflow and JAX. Provides thousands (25k) of pretrained models to perform tasks on various forms of data like text, vision, audio.\n",
    "\n",
    "\n",
    "### pipeline\n",
    "\n",
    "pipleines are easy to use interfces for models. they abstract most of the complex code from library, offering a simple API to perform taks luke Sentiment analysus, named entity recognition (a natural language processing (NLP) technique that identifies and categorizes information in text), masked language modeling(to predict missing words in a sentence).\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "- Extracting the answer from a context (question-answering).\n",
    "- Creating summaries from a large text (summarization).\n",
    "- Classify text (e.g. as spam or not spam, text-classification).\n",
    "- Generate a new text with models such as GPT (text-generation).\n",
    "- Identify parts of speech (verb, subject, etc.) or entities (country, organization, etc.) in a sentence (token-classification).\n",
    "- Transcribe audio files to text (automatic-speech-recognition).\n",
    "- Classify the speaker or language in an audio file (audio-classification).\n",
    "- Detect objects in an image (object-detection).\n",
    "- Segment an image (image-segmentation).\n",
    "- Reinforcement Learning (reinforcement-learning)\n",
    "\n",
    "\n",
    "Internally the pipeline does three things:\n",
    "\n",
    "1. Preprocessing the text using a tokenizer\n",
    "2. Feeds the preprocessed text to the model\n",
    "3. Post processing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tensorflow transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\TrainingMaterial\\generative-ai\\genai-material\\.venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "config.json: 100%|██████████| 998/998 [00:00<00:00, 499kB/s]\n",
      "c:\\TrainingMaterial\\generative-ai\\genai-material\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\KrishnaGopikaUrlagan\\.cache\\huggingface\\hub\\models--dbmdz--bert-large-cased-finetuned-conll03-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "model.safetensors: 100%|██████████| 1.33G/1.33G [06:29<00:00, 3.43MB/s]\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "tokenizer_config.json: 100%|██████████| 60.0/60.0 [00:00<00:00, 29.7kB/s]\n",
      "vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 397kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': 0.988891,\n",
       "  'index': 4,\n",
       "  'word': 'Go',\n",
       "  'start': 8,\n",
       "  'end': 10},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.8413752,\n",
       "  'index': 5,\n",
       "  'word': '##pi',\n",
       "  'start': 10,\n",
       "  'end': 12},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.7831352,\n",
       "  'index': 6,\n",
       "  'word': '##ka',\n",
       "  'start': 12,\n",
       "  'end': 14}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"ner\")\n",
    "\n",
    "classifier(\"Hi I Am Gopika\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9661751389503479}]\n"
     ]
    }
   ],
   "source": [
    "print(classifier(\"I am exicte to learn gen Ai!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Mt Everest is the tallest mountain the world',\n",
       " 'labels': ['Geographical', 'General', 'Political'],\n",
       " 'scores': [0.7967474460601807, 0.15498808026313782, 0.04826448857784271]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", candidate_labels=[\"Geographical\", \"Political\", \"General\"])\n",
    "\n",
    "classifier(\"Mt Everest is the tallest mountain the world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9993606209754944}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "\n",
    "\n",
    "classifier = pipeline('sentiment-analysis', model = model, tokenizer= tokenizer)\n",
    "\n",
    "\n",
    "classifier(\"Hey!\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 2572, 4083, 8991, 9932, 1010, 1045, 102]\n"
     ]
    }
   ],
   "source": [
    "enc_text = tokenizer.encode(\"I am learning Gen AI\")\n",
    "\n",
    "print(enc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] i am learning gen ai [SEP]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(enc_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Finetuning\n",
    "\n",
    "- prepare dataset\n",
    "- load a pretrained tokenizer, call it with dataset - encoding\n",
    "- load pretrained model\n",
    "- Trainer and Training Arguments(training dataset, evaluation dataset, data_collector, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(input_text):\n",
    "    return tokenizer(input_text['text'], padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[reference for padding and truncation](https://huggingface.co/docs/transformers/pad_truncation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:56<00:00, 444.74 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [01:00<00:00, 415.38 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [02:25<00:00, 343.88 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = dataset.map(tokenize_text, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_data['train'].shuffle(seed=42).select(range(1000))\n",
    "test_dataset = tokenized_data['test'].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install scikit-learn evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_mertric(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, reference=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer , TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments( output_dir=\"./base-uncased-eval\", evaluation_strategy=\"epoch\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_mertric\n",
    "\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./path-to-dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('C:/TrainingMaterial/generative-ai/genai-material/week1/bert-uncased-imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9978858828544617}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"Whoever wrote the screenplay for this movie obviously never consulted any books about Lucille Ball, especially her autobiography. I've never seen so many mistakes in a biopic, ranging from her early years in Celoron and Jamestown to her later years with Desi. I could write a whole list of factual errors, but it would go on for pages. In all, I believe that Lucille Ball is one of those inimitable people who simply cannot be portrayed by anyone other than themselves. If I were Lucie Arnaz and Desi, Jr., I would be irate at how many mistakes were made in this film. The filmmakers tried hard, but the movie seems awfully sloppy to me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9972653388977051}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"Master director Ching Siu Tung's perhaps most popular achievement is this series, A Chinese Ghost Story 1-3. Chinese Ghost Story stars Leslie Cheung in some distant past in China as a tax collector who is forced to spend a night during his 'collecting trip' in a mysterious castle in which some strange old warriors fight and meet him. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                      \n",
    " 33%|███▎      | 125/375 [2:00:31<1:37:11, 23.32s/it]{'eval_loss': 0.39614972472190857, 'eval_accuracy': 0.85, 'eval_runtime': 1363.0423, 'eval_samples_per_second': 0.734, 'eval_steps_per_second': 0.092, 'epoch': 1.0}\n",
    "                                                       \n",
    " 67%|██████▋   | 250/375 [3:27:41<1:05:16, 31.33s/it]{'eval_loss': 0.38924238085746765, 'eval_accuracy': 0.897, 'eval_runtime': 1458.4392, 'eval_samples_per_second': 0.686, 'eval_steps_per_second': 0.086, 'epoch': 2.0}\n",
    "                                                       \n",
    "100%|██████████| 375/375 [4:46:14<00:00, 45.80s/it]{'eval_loss': 0.4952440857887268, 'eval_accuracy': 0.903, 'eval_runtime': 1243.7463, 'eval_samples_per_second': 0.804, 'eval_steps_per_second': 0.101, 'epoch': 3.0}\n",
    "{'train_runtime': 17174.0601, 'train_samples_per_second': 0.175, 'train_steps_per_second': 0.022, 'train_loss': 0.2675406494140625, 'epoch': 3.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
